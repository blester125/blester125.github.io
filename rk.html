<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>R k correlation coefficient</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>
<p>
<div style="display:none">
  \[
    \newcommand{\cov}{\mathop{\rm cov}\nolimits}
    \newcommand{\mcc}{\mathop{\rm MCC}\nolimits}
    \newcommand{\R}{\mathbb{R}}
  \]
</div>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-t4zm{font-size:16px;border-color:inherit;text-align:center;vertical-align:middle}
.tg .tg-gmla{font-size:16px;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-7jts{font-size:18px;border-color:inherit;text-align:center;vertical-align:top}
</style>


Throughout this post we well have a confusion matrix that looks like this where the row index give the true label and the column index gives the predicted label.

<center>
<table class="tg">
  <tr>
    <th class="tg-0pky"></th>
    <th class="tg-gmla" colspan="2">Prediction</th>
  </tr>
  <tr>
    <td class="tg-t4zm" rowspan="2">Gold</td>
    <td class="tg-7jts">TP</td>
    <td class="tg-7jts">FN<br></td>
  </tr>
  <tr>
    <td class="tg-7jts">FP</td>
    <td class="tg-7jts">TN<br></td>
  </tr>
</table>
</center>

<h2>Simplifying the \(R_k\) calculation</h2>

\(R_k\) is the multiclass extension of the discretizated pearsons rank correlation and is defined as 

  \[
     R_k = \frac{\cov(X, Y)}{\sqrt{\cov(X, X)}\sqrt{\cov(Y, Y)}}
  \]

\(X\) and \(Y\) are one hot vectors \(\in \R^{S, N}\). Each row in the matrix represents a example in the dataset and each column represents a class in the problem. The row for example \(s\) has a 1 in column \(k\) if the label for that example is \(k\) and is zero otherwise.

  \[
     \cov(X, Y) = \sum_k^N w_k \cov(X_k, Y_k)
  \]

\(w_k\) is a weight on some class \(k\), we use a uniform prior on all classes so \(w_k = \frac{1}{N}\). Because it is a constant across the summation we can pull it out (it is a multiplication which distributes over addition).
<br/>
<br/>

Using the definition of covariance we get that

  \[
     \cov(X, Y) = w_k \sum_s^S \sum_k^N (X_{sk} - \bar{X}_k)(Y_{sk} - \bar{Y}_k)
  \]

  \(\bar{X}_k\) is the mean of the \(k^{th}\) column of of \(X\). Because \(X\) is a one hot where a column \(k\) for a row \(s\) is one if that examples was labeled with \(k\) we can read this value from the confusion matrix. It is the sum of all elements in column \(k\) of the confusion matrix because that is the class in \(X\) regardless of the class in \(\bar{Y}\). The denominator in the mean is the total number of samples because the mean is actually over all \(S\) rows in \(X\) even if we read the values from the confusion matrix.

  \[
     \bar{X}_k = \frac{1}{S}\sum_s^S X_{sk} = \frac{1}{S}\sum_l^N C_{lk}
  \]

We can do the same transformation into a confusion matrix based calculation of \(Y\) except that the sum is along a row because we care about the value in \(Y\) regardless of the value in \(X\).

  \[
     \bar{Y}_k = \frac{1}{S}\sum_s^S Y_{sk} = \frac{1}{S}\sum_l^N C_{kl}
  \]

Foil the covariance calculation

  \[
     \cov(X, Y) = w_k \sum_s^S \sum_k^N X_{sk}Y_{sk} - X_{sk}\bar{Y}_k - Y_{sk}\bar{X}_k + \bar{X}_k \bar{Y}_k
  \]

thinking of the \(- x\) in the above as \(+ -1 * x\) we can see how we are allowed to rearrange the order of the summations that allow use to put the sum in front of each term so we can reason about it individually

  \[
     \cov(X, Y) = w_k \sum_s^S \sum_k^N X_{sk}Y_{sk} - \sum_s^S \sum_k^N X_{sk}\bar{Y}_k - \sum_s^S \sum_k^N Y_{sk}\bar{X}_k + \sum_s^S \sum_k^N \bar{X}_k \bar{Y}_k
  \]

Lets start with \(\sum\nolimits_s^S \sum_k^N \bar{X}_k \bar{Y}_k\).

First we can see the \(s\) variable is not used, this means that the value calculated in the summation over \(k\) is repeated \(s\) times. This is the definition of multiplication (repeated addition)

  \[
     \sum_s^S \sum_k^N \bar{X}_k \bar{Y}_k = S\sum_k^N \bar{X}_k \bar{Y}_k
  \]

Then we can substitute in out definitions of \(\bar{X}_k\) and \(\bar{Y}_k\) for earlier.

  \[
     \sum_s^S \sum_k^N \bar{X}_k \bar{Y}_k = S\sum_k^N \frac{1}{S}\sum_l^N C_{lk} \frac{1}{S}\sum_l^N C_{kl}
  \]

We can pull the divisions by \(S\) out because division is multiplication by the reciprocal and multiplication distributes over the addition.

  \[
     \sum_s^S \sum_k^N \bar{X}_k \bar{Y}_k = \frac{S}{S^2}\sum_k^N \sum_l^N C_{lk} \sum_l^N C_{kl}
  \]

The \(S\) in the numerator cancels with one of the \(S\)s in the denominator.

  \[
     \sum_s^S \sum_k^N \bar{X}_k \bar{Y}_k = \frac{1}{S}\sum_k^N \sum_l^N C_{lk} \sum_l^N C_{kl}
  \]



From a math perspective there isn't a lot to do with the first term, we'll come back to this when we convert it to code. \(\sum_s^S \sum_k^N X_{sk}Y_{sk}\)

<br/>
<br/>

Now lets take a look at \(\sum_s^S \sum_k^N X_{sk}\bar{Y}_k\). Similar to above we can pull \(\bar{Y}_k\) out of the summation over \(s\) because \(\bar{Y}_k\) doesn't depend on \(s\)

  \[
    \sum_s^S \sum_k^N X_{sk} \bar{Y}_k = \sum_k^N \bar{Y}_k \sum_s^S X_{sk}
  \]

\(\sum_s^S X_{sk}\) is the number of examples in \(X\) that belong to class \(k\). Can be read from the confusion matrix.

  \[
	\sum_s^S X_{sk} = \sum_k^N C_{lk}
  \]

Substituting both this definition and the confusion matrix based definition of \(\bar{Y}_k\) we get:

  \[
    \sum_s^S \sum_k^N X_{sk} \bar{Y}_k = \sum_k^N  \sum_l^N C_{lk} \frac{1}{S}\sum_l^N C_{kl}
  \]

We can also pull the multiplication by \(\frac{1}{S}\) out of the summation (multiplication distributes over addition once again)

  \[
    \sum_s^S \sum_k^N X_{sk} \bar{Y}_k = \frac{1}{S} \sum_k^N \sum_l^N C_{lk} \sum_l^N C_{kl}
  \]

\(\sum_s^S \sum_k^N Y_{sk}\bar{X}_k\) can be expanded similarly

  \[
    \sum_s^S \sum_k^N Y_{sk}\bar{X}_k = \frac{1}{S} \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{lk}
  \]

Substituting these expansions into our foiled expression we get

  \[
     \cov(X, Y) = w_k \sum_s^S \sum_k^N X_{sk}Y_{sk} - \frac{1}{S} \sum_k^N \sum_l^N C_{lk} \sum_l^N C_{kl} - \frac{1}{S} \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{lk} + \frac{1}{S}\sum_k^N \sum_l^N C_{lk} \sum_l^N C_{kl}
  \]

Here we can see that the <code>outer</code> and <code>last</code> sections cancel out.

  \[
     \cov(X, Y) = w_k \sum_s^S \sum_k^N X_{sk}Y_{sk} - \frac{1}{S} \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{lk}
  \]

Expanding \(w_k\) as can rewrite as 

  \[
     \cov(X, Y) = \frac{\sum_s^S \sum_k^N X_{sk}Y_{sk} -  \frac{\sum_k^N \sum_l^N C_{kl} \sum_l^N C_{lk}}{S}}{k}
  \]

We can multiply by \(\frac{S}{S}\) because it the same as multiplying by \(1\).

  \[
     \cov(X, Y) = \frac{\sum_s^S \sum_k^N X_{sk}Y_{sk} -  \frac{\sum_k^N \sum_l^N C_{kl} \sum_l^N C_{lk}}{S}}{k} * \frac{S}{S}
  \]

The \(S\) in the numerator distributes over the subtraction. It cancels with the \(S\) in the denominator of the second term.


  \[
     \cov(X, Y) = \frac{S\sum_s^S \sum_k^N X_{sk}Y_{sk} - \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{lk}}{k * S}
  \]

When we calculate \(\cov(X, X)\) we have can apply the same math, this time however we don't have \(\bar{Y}_k\) so all the confusion matrix definitions are \(C_{lk}\)

  \[
     \cov(X, X) = \frac{S\sum_s^S \sum_k^N X_{sk}X_{sk} - \sum_k^N \sum_l^N C_{lk} \sum_l^N C_{lk}}{k * S}
  \]

Similarly \(\cov(Y, Y)\) only has confusion matrix definitions that look like \(C_{kl}\)

  \[
     \cov(Y, Y) = \frac{S\sum_s^S \sum_k^N Y_{sk}Y_{sk} - \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{kl}}{k * S}
  \]

Lets plug this back into the \(R_k\) definition.

  \[
     R_k = \frac{
		\frac{S\sum_s^S \sum_k^N X_{sk}Y_{sk} - \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{lk}}{k * S}
	}{
		\sqrt{\frac{S\sum_s^S \sum_k^N X_{sk}X_{sk} - \sum_k^N \sum_l^N C_{lk} \sum_l^N C_{lk}}{k * S}}\sqrt{\frac{S\sum_s^S \sum_k^N Y_{sk}Y_{sk} - \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{kl}}{k * S}}
	}
  \]

Using some rules of square roots \(\sqrt{a}\sqrt{b} = \sqrt{ab}\) and \(\sqrt{\frac{a}{b}} = \frac{\sqrt{a}}{\sqrt{b}}\) we can re-write \(R_k\) as

  \[
     R_k = \frac{
		\frac{S\sum_s^S \sum_k^N X_{sk}Y_{sk} - \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{lk}}{k * S}
	}{
		\frac{\sqrt{S\sum_s^S \sum_k^N X_{sk}X_{sk} - \sum_k^N \sum_l^N C_{lk} \sum_l^N C_{lk} * S\sum_s^S \sum_k^N Y_{sk}Y_{sk} - \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{kl}}}{\sqrt{(k*S)^2}}
	}
  \]

Because \(\sqrt{a^2} = a\) can drop the square root on the bottom.

  \[
     R_k = \frac{
		\frac{S\sum_s^S \sum_k^N X_{sk}Y_{sk} - \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{lk}}{k * S}
	}{
		\frac{\sqrt{S\sum_s^S \sum_k^N X_{sk}X_{sk} - \sum_k^N \sum_l^N C_{lk} \sum_l^N C_{lk} * S\sum_s^S \sum_k^N Y_{sk}Y_{sk} - \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{kl}}}{k*S}
	}
  \]

Division is multiplication by the reciprocal so we can rewrite like so

  \[
     R_k =
		\frac{S\sum_s^S \sum_k^N X_{sk}Y_{sk} - \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{lk}}{k * S} *
		\frac{k * S}{\sqrt{S\sum_s^S \sum_k^N X_{sk}X_{sk} - \sum_k^N \sum_l^N C_{lk} \sum_l^N C_{lk} * S\sum_s^S \sum_k^N Y_{sk}Y_{sk} - \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{kl}}}
  \]

The \(k * S\)'s cancel leaving us with a final simplified \(R_k\) of

  \[
     R_k =
		\frac{S\sum_s^S \sum_k^N X_{sk}Y_{sk} - \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{lk}}
		{\sqrt{S\sum_s^S \sum_k^N X_{sk}X_{sk} - \sum_k^N \sum_l^N C_{lk} \sum_l^N C_{lk} * S\sum_s^S \sum_k^N Y_{sk}Y_{sk} - \sum_k^N \sum_l^N C_{kl} \sum_l^N C_{kl}}}
  \]

This still looks fairly complicated and we have this annoying disconnected where the majority of elements are calculated from the confusion matrix \(C\) but we have some things \(\sum_s^S X_{sk}Y_{sk}\), \(\sum_s^S X_{sk}X_{sk}\) \(\sum_s^S Y_{sk}Y_{sk}\) are calculated based on the one hot representation, this representation is a waste of memory and building it is a waste of time. We would like to be able to calculate this based on the confusion matrix too.


<h2>Converting to numpy</h2>

Lets start translating these into numpy code. Lets start with the simple ones.

<ul>
	<li> \(S\) is the number of examples in the dataset. This is an easy translation numpy, <code>np.sum(C)</code> </li>
    <li> \(\sum_l^N C_{kl}\) produces a vector which has the sum of each column. <code>np.sum(C, axis=0)</code> </li>
    <li> \(\sum_l^N C_{lk}\) produces a vector which has the sum of each row. <code>np.sum(C, axis=1)</code> </li>
 	<li> \(\sum_k^N \sum_l^N C_{kl} \sum_l^N C_{kl}\) takes the sum of the product between each element in the vectors. This is the definition of the dot product.</li>
    <ul>
		<li>\(\sum_k^N \sum_l^N C_{kl} \sum_l^N C_{lk}\) from \(\cov(X, Y)\) translates to <code>np.dot(np.sum(C, axis=1), np.sum(C, axis=0))</code></li>
		<li>\(\sum_k^N \sum_l^N C_{lk} \sum_l^N C_{lk}\) from \(\cov(X, X)\) translates to <code>np.dot(np.sum(C, axis=0), np.sum(C, axis=0))</code></li>
		<li>\(\sum_k^N \sum_l^N C_{kl} \sum_l^N C_{kl}\) from \(\cov(Y, Y)\) translates to <code>np.dot(np.sum(C, axis=1), np.sum(C, axis=1))</code></li>
	</ul>
</ul>

<br/>
<br/>



Transforming \(\sum_s^S \sum_k^N X_{sk}Y_{sk}\) is a little trickier and depends on understanding what these matrices represent. \(X\) and \(Y\) are one hot matrices \(\in \R^{s,k}\) where \(X_{sk}\) is \(1\) if example \(s\) was classified as class \(k\). Because these are one hot most of the values in the product are a zero, the value is only one when \(sk\) is one for both matrices. When the elements in this expression are \(X\) and \(Y\) there is only a value when the classification for that example matches. This can be read from the confusion matrix, the counts on the diagonal are the number of examples for each class where the gold label matches the predicted label. This sum can be expressed as \(\sum_k^N C_{kk}\) and as <code>np.sum(np.diagonal(C))</code> or <code>np.trace(C)</code> in code.

<br/>
<br/>

When the inputs match (both are \(X\) or both are \(Y\)) the elements at a given \(sk\) will always match. Because these are one hots the value of the multiplication is always one and there is only a single class that has a one in it. This means this sum will always just be the number of examples. This can be pull from the confusion matrix with <code>np.sum(C)</code>

<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

A second way to look at \(\sum_s^S \sum_k^N X_{sk}\bar{Y}_k\) is as follows. First remember the definition of \(\bar{X}_k\)

  \[
     \bar{X}_k = \frac{1}{S}\sum_s^S X_{sk}
  \]
  \[
     S\bar{X}_k = \sum_s^S X_{sk}
  \]

This means we can substitute this in and pull the \(S\) out of the sum

  \[
     S\sum_k^N \bar{X}_k \bar{Y}_k
  \]

Using this substitution for both the <code>outer</code> and <code>inner</code> sections as well as our fist transformation of the <code>last</code> section we can see how the terms cancel.

  \[
     \cov(X, Y) = w_k \sum_s^S \sum_k^N X_{sk}Y_{sk} - S \sum_k^N \bar{X}_k\bar{Y}_k - S \sum_k^N \bar{Y}_k\bar{X}_k + S \sum_k^N \bar{X}_k \bar{Y}_k
  \]

  \[
     \cov(X, Y) = w_k \sum_s^S \sum_k^N X_{sk}Y_{sk} - S \sum_k^N \bar{Y}_k\bar{X}_k
  \]

We can then use the transformations discussed above to define each term by the confusion matrix.


<h2>Reduction to Matthews Correlation Coefficient for k=2</h2>

The Wikipedia page for MCC gives multiple ways to calculate it by reading from the confusion matrix and also the original equations that Matthews gave in his paper. This presentation skips however the original formula Matthews presented for the correlation between prediction \(P_n\) (what we would call \(X_s\)) and observation \(S_n\) (what we call \(Y_s\)). This is the formula converted to use the same variables we do.

\[
    \mcc(X, Y) = \frac{\sum_s^S (Y_s - \bar{Y}) (X_s - \bar{X})}{\sqrt{\sum_s^S (X_s - \bar{X})^2 \sum_s^S (Y_s - \bar{Y})^2}}
\]

\(\sum_s^S (X_s - \bar{X})^2\) expands to \(\sum_s^S (X_s - \bar{X})(X_s - \bar{X})\) so we can rewrite this as

\[
    \mcc(X, Y) = \frac{\sum_s^S (Y_s - \bar{Y}) (X_s - \bar{X})}{\sqrt{\sum_s^S (X_s - \bar{X})(X_s - \bar{X})\sum_s^S (Y_s - \bar{Y})(Y_s - \bar{Y}) }}
\]

The we can see that this is the covariance calculations we calculated earlier!

\[
    \mcc(X, Y) = \frac{\cov(Y, X)}{\sqrt{\cov(X, X)\cov(Y, Y)}}
\]


\(\cov\) is symmetric so \(\cov(Y, X) = \cov(X, Y)\) meaning we can write this as

\[
    \mcc(X, Y) = \frac{\cov(X, Y)}{\sqrt{\cov(X, X)\cov(Y, Y)}}
\]


Now we see that this is the same things calculated for \(R_k\)! The only different is that \(R_k\) is calculated for multiple classes in this case \(k=2\) while the original MCC calculation is based on classes of \(0\) and \(1\) and is there for calculated with a single vector \(X \in \R^S\) opposed to \(R_k\) which has \(X \in \R^S\)

This part is a little odd and I don't have a great way to demonstrate this mathematically but we can see these are the same. Fist imagine that we have some vector of labels with two classes \(X \in \R^s\)

  \[
    \begin{bmatrix}
        1 \\
        0 \\
        1 \\
        1 \\
        \vdots \\
        0
    \end{bmatrix}
  \]

Now we convert it into a one hot representation where the value in column \(k\) is \(1\) if the example is labeled \(k\) and \(0\) otherwise. \(X' \in \R^{s, k}\)

  \[
    \begin{bmatrix}
        0 & 1 \\
        1 & 0 \\
        0 & 1 \\
        0 & 1 \\
        \vdots & \vdots \\
        1 & 0
    \end{bmatrix}
  \]

 What we see is that the column \(1\) is the exact same as out original vector and column zero is the same but flipped. Now image we have two vectors \(X\) and \(Y\) and we calculate \(\alpha = \mcc(X, Y)\). Now we know that the calculation for \(R_k\) is same as for \(MCC\) applied to each row followed by a linear combination weighted by some weight for each class \(w_k\) where we have defined \(w_k = \frac{1}{N}\). Now looking at the one hot representation we see that for the \(k=1\) column the inputs are the same. This means that \(R_1 = \alpha\). For the \(k = 0\) column we have the same numbers but flipped. Because everything is just one's and zeros things like our sums and means are the same meaning that \(R_0 = \alpha\) also. Now we multiply by \(w_k\) and sum.

 \[
    \begin{align}
    w_k &= \frac{1}{2} \\
    R_k &= w_k * \alpha + w_k * \alpha \\
    &= w_k (\alpha + \alpha) \\
    &= w_k * 2\alpha \\
    &= \frac{2 * \alpha}{2} \\
    R_k &= \alpha \\
    \end{align}
 \]

Here we can see that calculation \(R_k\) reduces to the calculation for \(MCC\) when \(k = 2\).


</p>
</body>
</html>
