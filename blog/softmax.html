<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>Numberically Stable Softmax</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <!--- Custom Code --->
  <script src="../js/interactive.js"></script>
  <script type='text/javascript'>
    window.onload = addHoverToLines.bind(null, 'show-line-')
  </script>
  <link rel="stylesheet" type="text/css" href="../css/teaching.css">

</head>
<body>
<div class="content">
  <div class="math-def">
    \[
      \newcommand{\softmax}{\mathop{\rm softmax}\nolimits}
      \newcommand{\qed}{\tag*{$\Box$}}
    \]
  </div>

  <div class="left">
    \[
      \begin{align}
        \cssId{show-line-1}\softmax(x) &= \frac{e^{x_i}}{\sum_j e^{x_j}} \\
        \cssId{show-line-2}\softmax(x) &= \softmax(x - \alpha) \\
        \cssId{show-line-3}\alpha &= \max_i x_i
      \end{align}
    \]

    \[
      \begin{align}
        \cssId{show-line-4}\softmax(x-\alpha) &= \frac{e^{x_i - \alpha}}{\sum_j e^{x_j - \alpha}} \\
        \cssId{show-line-5}\softmax(x_i) &= \frac{e^{x_i}}{\sum_j e^{x_j}} \\
        &= \cssId{show-line-6}{ \frac{e^{-\alpha}}{e^{-\alpha}} * \frac{e^{x_i}}{\sum_j e^{x_j}} }\\
        &= \cssId{show-line-7}{ \frac{e^{-\alpha}e^{x_i}}{e^{-\alpha}\sum_j e^{x_j}} }\\
        &= \cssId{show-line-8}{ \frac{e^{-\alpha}e^{x_i}}{\sum_j e^{-\alpha} e^{x_j}} }\\
        &= \cssId{show-line-9}{ \frac{e^{-\alpha + x_i}}{\sum_j e^{-\alpha + x_j}} }\\
        &= \cssId{show-line-10}{ \frac{e^{x_i -\alpha}}{\sum_j e^{x_j -\alpha}} }\\
        \cssId{show-line-11} \softmax(x) &= \softmax(x - \alpha) \qed
      \end{align}
    \]
  </div>

  <div class="right">
    <ul class="explain">
      <li id="explain-line-1"> This is the definition of softmax, it takes a vector x and exponentiates each element. It then normalizes each element with the sum of all elements </li>
      <li id="explain-line-2"> This is the equality we are trying to prove </li>
      <li id="explain-line-3"> This is the definition of \( \alpha \) we are using but this could be any arbitrary number </li>
      <li id="explain-line-4"> This is the same definition of softmax but we include the subtraction of alpha. This is the value we want to transform the softmax equation into.
      <li id="explain-line-5"> Again this is our definition </li>
      <li id="explain-line-6"> Because anything divided by itself is 1 and any number multiplied by 1 is itself (multiplicity identity) we can introduce this e to the negative alpha without changing anything. </li>
      <li id="explain-line-7"> A fraction multiplied by a fraction is the numerators multiplied together and the denominators multiplied together.</li>
      <li id="explain-line-8"> Multiplication distributes over the addition in the summation so we can push the \( e^{-\alpha} \) into the summation </li>
      <li id="explain-line-9"> The rules of exponents says when multiplying elements that have the same base we sum the powers. </li>
      <li id="explain-line-10"> Addition is commutative so we can move the \( -\alpha \) after the \( x_i \) </li>
      <li id="explain-line-11"> We showed that both \(\softmax(x)\) and \(\softmax(x - \alpha)\) are equal to the same things so by the transitive property they are equal to each other. </li>
    </ul>
  </div>
</div>

</body>
</html>
