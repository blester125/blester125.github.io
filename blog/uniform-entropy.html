<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width" />

    <meta
      name="og:description"
      content="How Entropy Measures Change when you are working with a Uniform Distribution"
    />
    <meta name="og:title" content="KL Divergence and Uniform Distributions." />
    <meta
      name="og:url"
      content="https://blester125.com/blog/uniform-entropy.html"
    />
    <meta
      name="og:image"
      content="https://blester125.com/static/img/blog/.png"
    />
    <meta name="twitter:card" content="summary_large_image" />

    <meta name="author" content="Brian Lester" />
    <title>KL Divergence and Uniform Distributions.</title>

    <!-- Third Party CSS -->
    <link
      href="https://stackpath.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7"
      crossorigin="anonymous"
    />

    <!-- Fonts -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0/css/all.min.css"
      integrity="sha256-ybRkN9dBjhcS2qrW1z+hfCxq+1aBdwyQM5wlQoQVt/0="
      crossorigin="anonymous"
    />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Montserrat:400,700"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Kaushan+Script"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700"
      rel="stylesheet"
      type="text/css"
    />

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js" integrity="sha384-0s5Pv64cNZJieYFkXYOTId2HMA2Lfb6q2nAcx2n0RTLUnCAoTTsS0nKEO27XyKcY" crossorigin="anonymous"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js" integrity=sha384-ZoaMbDF+4LeFxg6WdScQ9nnR1QC2MIRxA1O9KWEXQwns1G8UNyIEZIQidzb0T1fo" crossorigin="anonymous"></script>
    <![endif]-->

    <!-- favicon, the logo in the tab -->
    <link rel="icon" href="favicon.ico" type="image/x-icon" />
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />

    <!-- Third Party Javascript -->
    <script
      src="https://code.jquery.com/jquery-1.11.1.min.js"
      integrity="sha256-VAvG3sHdS5LqTT+5A/aeq/bZGa/Uj04xKxY8KM/w9EE="
      crossorigin="anonymous"
    ></script>
    <script
      src="https://stackpath.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"
      integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS"
      crossorigin="anonymous"
    ></script>
    <script src="../static/js/vendored/jquery.scrollintoview.min.js"></script>

    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js"
      crossorigin="anonymous"
    ></script>

    <!-- Custom CSS -->
    <link href="/static/css/agency.css" rel="stylesheet" />
    <link href="/static/css/teaching.css" rel="stylesheet" type="text/css" />
    <link href="/static/css/nav.css" rel="stylesheet" />
    <link href="/static/css/nav-placeholder.css" rel="stylesheet" />

    <!-- Custom JS -->
    <script src="/static/js/interactive.js"></script>
    <script src="/static/js/fetch.js"></script>
    <script type="text/javascript">
      window.onload = addHoverToLines.bind(null, "show-line-", 10, 0.01);
      $(function () {
        $("#nav").load("/static/nav.html", function () {
          loadScript("/static/js/agency.js");
        });
        $("#footer").load("/static/footer.html", function () {
          $(".social-links").load("/static/socials.html");
        });
      });
    </script>

    <!-- prettier-ignore -->
    <div class="math-def">
      \[
        \newcommand{\softmax}{\mathop{\rm softmax}\nolimits}
        \newcommand{\qed}{\tag*{$\Box$}}
      \]
    </div>
  </head>
  <body class="bg-light-gray">
    <div id="nav" class="navbar-placeholder"></div>

    <div class="container">
      <div class="content">
        <div class="section" id="title">
          <center>
            <h1>Entropy and a Uniform Distribution</h1>
          </center>
        </div>
        <div class="row">
          <div class="section col-lg-8 col-lg-offset-2" id="introduction">
            <p>Introduce the Setting</p>
          </div>
        </div>

        <div class="row">
          <div class="section" id="entropy">
            <center>
              <h2>Definition of Entropy</h2>
            </center>
          </div>
        </div>
        <div class="row">
          <!-- prettier-ignore -->
          <div class="math">
            <div class="math-overflow">
              \[
                \begin{align}
                  \cssId{show-line-1}H[P] &= \mathbb{E}[\log\frac{1}{p(x)}] \\
                  &=\cssId{show-line-2}{\mathbb{E}[\log 1 - \log p(x)]} \\
                  &=\cssId{show-line-3}{\mathbb{E}[0 - \log p(x)]} \\
                  &=\cssId{show-line-4}{\mathbb{E}[- \log p(x)]} \\
                  &=\cssId{show-line-5}{\sum_{x\in\mathcal{X}}p(x) * - \log p(x)} \\
                  \cssId{show-line-6} H[P] &= -\sum_{x\in\mathcal{X}}p(x) \log p(x) \qed \\
                \end{align}
              \]
            </div>
          </div>

          <div class="explaination">
            <ul class="explain">
              <li id="explain-line-1">
                This is the original definition of entropy that is related to
                compression. The best compression for a symbol with a
                probability \(p(x)\) is a code of length \(\log \frac{1}{p}\)
                \cite{}. Therefore the amount of information can be viewed as
                the expectation of the length when perfectly compressed.
              </li>
              <li id="explain-line-2">
                Apply the Rule of Logs, \(\log \frac{A}{B} = \log A - \log B\)
              </li>
              <li id="explain-line-3">\(\log 1 = 0\)</li>
              <li id="explain-line-4">\(0\) is the additive identity.</li>
              <li id="explain-line-5">
                The expectation for a discrete random variable is the sum of the
                the probability of some value times the function applied to that
                value for all possible values.
              </li>
              <li id="explain-line-6">
                Multiplication distributes over addition, so we can pull the
                \(-1\) out of the summation. We have now arrived at the equation
                that people in ML generally use for entropy.
              </li>
            </ul>
          </div>
        </div>
        <div class="row">
          <div class="section" id="entropy">
            <center>
              <h2>The Entropy of a Uniform Distribution</h2>
            </center>
          </div>
        </div>
        <div class="row">
          <!-- prettier-ignore -->
          <div class="math">
            <div class="math-overflow">
              \[
                \begin{align}
                  \cssId{show-line-7}H[P] &= -\sum_{x\in\mathcal{X}} p(x) \log p(x) \\
                  \cssId{show-line-8}u(x) &= \frac{1}{|\mathcal{X}|} \\
                  \cssId{show-line-9}U &= \forall_{x \in \mathcal{X}}\text{  } p(x) = \frac{1}{|\mathcal{X}|}
                \end{align}
              \]
            </div>

            <div class="math-overflow">
              \[
                \begin{align}
                  \cssId{show-line-10}H[U] &= -\sum_{x\in\mathcal{X}} u(x) \log u(x) \\
                  \cssId{show-line-11}H[U] &= -\sum_{x\in\mathcal{X}} \frac{1}{|\mathcal{X}|} \log \frac{1}{|\mathcal{X}|} \\
                  &= \cssId{show-line-12}{ -\sum_{x\in\mathcal{X}} \frac{1}{|\mathcal{X}|} (\log 1 - \log |\mathcal{X}|) }\\
                  &= \cssId{show-line-13}{ -\sum_{x\in\mathcal{X}} \frac{1}{|\mathcal{X}|}\log(1) - \frac{1}{|\mathcal{X}|}\log |\mathcal{X}| }\\
                  &= \cssId{show-line-14}{ -\sum_{x\in\mathcal{X}} \frac{1}{|\mathcal{X}|} * 0 - \frac{1}{|\mathcal{X}|}\log |\mathcal{X}| }\\
                  &= \cssId{show-line-15}{ -\sum_{x\in\mathcal{X}} -\frac{1}{|\mathcal{X}|}\log |\mathcal{X}| }\\
                  &= \cssId{show-line-16}{ \sum_{x\in\mathcal{X}} \frac{1}{|\mathcal{X}|}\log |\mathcal{X}| }\\
                  &= \cssId{show-line-17}{ \frac{1}{|\mathcal{X}|}\sum_{x\in\mathcal{X}} \log |\mathcal{X}| }\\
                  &= \cssId{show-line-18}{ \frac{1}{|\mathcal{X}|} * |\mathcal{X}| * \log |\mathcal{X}| }\\
                  \cssId{show-line-19}H[U] &= \log |\mathcal{X}| \qed \\
                \end{align}
              \]
            </div>
          </div>

          <div class="explaination">
            <ul class="explain">
              <li id="explain-line-7">
                This is the definition of entropy we found above.
              </li>
              <li id="explain-line-8">
                In a uniform distribution, the probability for a given symbol is
                \(1\) divided by the number of possible symbols.
              </li>
              <li id="explain-line-9">
                For a uniform distribution, all symbols have the same
                probability.
              </li>
              <li id="explain-line-10">
                The entropy of a uniform distribution.
              </li>
              <li id="explain-line-11">We plug in the values for \(u(x)\).</li>
              <li id="explain-line-12">
                Apply the rule of logs, \(\log \frac{A}{B} = \log A - \log B\).
              </li>
              <li id="explain-line-13">
                Distribution \(\frac{1}{|\mathcal{X}|}\)
              </li>
              <li id="explain-line-14">\(\log 1 = 0\)</li>
              <li id="explain-line-15">
                \(0\) multiplied by anything is \(0\).
              </li>
              <li id="explain-line-16">
                Multiplication distributes over addition so we can pull the
                \(-1\) out of the summation.
              </li>
              <li id="explain-line-17">
                Multiplication distributes over addition so we can pull the
                \(\frac{1}{|\mathcal{X}|}\) out of the summation.
              </li>
              <li id="explain-line-18">
                As the value in the summation, \(\log |\mathcal{X}|\), does not
                depend on \(x\), the result of the summation will be the
                addition of it \(|\mathcal{X}|\) times. Multiplication is
                repeated addition, thus the summation is the same as multiplying
                by \(|\mathcal{X}\).
              </li>
              <li id="explain-line-19">
                Multiplication by the reciprocal always yields \(1\) and \(1\)
                is the multiplicative identity. Therefore we are just left with
                the Entropy of the Uniform distribution as the log of the
                vocabulary size.
              </li>
            </ul>
          </div>
        </div>

        <div class="row">
          <div class="section" id="entropy">
            <center>
              <h2>KL Divergence from a Uniform Distribution</h2>
            </center>
          </div>
        </div>
        <div class="row">
          <!-- prettier-ignore -->
          <div class="math">
            <div class="math-overflow">
              \[
                \begin{align}
                  \cssId{show-line-20}KL(P || Q) &= \sum_{x\in\mathcal{X}} p(x) \log\frac{p(x)}{q(x)} \\
                  \cssId{show-line-21}H[P] &= -\sum_{x\in\mathcal{X}} p(x) \log p(x) \\
                  \cssId{show-line-22}H[U] &= \log |\mathcal{X}| \\
                  \cssId{show-line-23}u(x) &= \frac{1}{|\mathcal{X}|} \\
                  \cssId{show-line-24}\sum_{x\in\mathcal{X}} p(x) &= 1
                \end{align}
              \]
            </div>

            <div class="math-overflow">
              \[
                \begin{align}
                  \cssId{show-line-25}KL(P || U) &= \sum_{x\in\mathcal{X}} p(x) \log\frac{p(x)}{u(x)} \\
                  &= \cssId{show-line-26}{\sum_{x\in\mathcal{X}} p(x) (\log p(x) - \log u(x))} \\
                  &= \cssId{show-line-27}{\sum_{x\in\mathcal{X}} p(x)\log p(x) - p(x)\log u(x)} \\
                  &= \cssId{show-line-28}{\sum_{x\in\mathcal{X}} p(x)\log p(x) - \sum_{x\in\mathcal{X}} p(x)\log u(x)} \\
                  &= \cssId{show-line-29}{-H[P] - \sum_{x\in\mathcal{X}} p(x)\log u(x)} \\
                  &= \cssId{show-line-30}{-H[P] - \sum_{x\in\mathcal{X}} p(x)\log \frac{1}{|\mathcal{X}|}} \\
                  &= \cssId{show-line-31}{-H[P] - \sum_{x\in\mathcal{X}} p(x)(\log 1 - \log |\mathcal{X}|)} \\
                  &= \cssId{show-line-32}{-H[P] - \sum_{x\in\mathcal{X}} p(x)\log 1 - p(x)\log |\mathcal{X}|} \\
                  &= \cssId{show-line-33}{-H[P] - \sum_{x\in\mathcal{X}} p(x) * 0 - p(x)\log |\mathcal{X}|} \\
                  &= \cssId{show-line-34}{-H[P] - \sum_{x\in\mathcal{X}} - p(x)\log |\mathcal{X}|} \\
                  &= \cssId{show-line-35}{-H[P] + \sum_{x\in\mathcal{X}} p(x)\log |\mathcal{X}|} \\
                  &= \cssId{show-line-36}{-H[P] + \log |\mathcal{X}| \sum_{x\in\mathcal{X}} p(x)} \\
                  &= \cssId{show-line-37}{-H[P] + \log |\mathcal{X}| * 1} \\
                  &= \cssId{show-line-38}{-H[P] + \log |\mathcal{X}|} \\
                  &= \cssId{show-line-39}{\log |\mathcal{X}| -H[P]} \\
                  \cssId{show-line-40}KL(P || U) &= H[U] - H[P] \qed \\
                \end{align}
              \]
            </div>
          </div>

          <div class="explaination">
            <ul class="explain">
              <li id="explain-line-20">The definition of KL Divergence.</li>
              <li id="explain-line-21">
                The definition of entropy we estabilished above.
              </li>
              <li id="explain-line-22">
                The entropy of the uniform distirbution we established above.
              </li>
              <li id="explain-line-23">
                The probability of some symbol in a uniform distribution.
              </li>
              <li id="explain-line-24">
                For a probability distribution to be valid, it must sum to
                \(1\).
              </li>
              <li id="explain-line-25">
                Our KL divergence between some distribution \(P\) and the
                uniform distribution.
              </li>
              <li id="explain-line-26">
                Apply the rule of logs, \(\log \frac{A}{B} = \log A - \log B\).
              </li>
              <li id="explain-line-27">
                Distribute multiplication of \(p(x)\).
              </li>
              <li id="explain-line-28">Linearity of the Sum.</li>
              <li id="explain-line-29">
                Based on our definition above, the first term in the negative
                entropy of \(P\).
              </li>
              <li id="explain-line-30">
                Plug in the probability given by the uniform distribution.
              </li>
              <li id="explain-line-31">
                Apply the rule of logs, \(\log \frac{A}{B} = \log A - \log B\).
              </li>
              <li id="explain-line-32">Distribute \(p(x)\).</li>
              <li id="explain-line-33">\(\log 1 = 0\).</li>
              <li id="explain-line-34">
                \(0\) multiplied by anything is zero and zero is the additive
                identity.
              </li>
              <li id="explain-line-35">
                Multiplication distributes over addition, thus we can pull the
                \(-1\) out of the summation.
              </li>
              <li id="explain-line-36">
                \(\log |\mathcal{X}|\) does not depend on the value of \(x\),
                thus we pull it out of the summation, again because
                multiplication distributes over addition.
              </li>
              <li id="explain-line-37">
                Based on the definition of a valid probability distribution, the
                final term is equal to \(1\).
              </li>
              <li id="explain-line-38">
                \(1\) is the multiplicative identity.
              </li>
              <li id="explain-line-39">
                Addition is commutative, so we can rearrange the terms.
              </li>
              <li id="explain-line-40">
                Above we found that \(\log |\mathcal{X}|\) is the entropy of the
                uniform distribution. Therefore, we have found that the KL
                Divergence between some distribution \(P\) and the uniform
                distribution is given by the entropy of the uniform distribution
                minus the entropy of \(P\).
              </li>
            </ul>
          </div>
        </div>

        <div class="row">
          <div class="section col-lg-8 col-lg-offset-2" id="conclusion">
            <p>
              Now we can always refer back to this is I ever dobut myself lol.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div id="footer"></div>
  </body>
</html>
